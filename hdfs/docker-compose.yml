services:
  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./config
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    # volumes:
    #   - hadoop_namenode:/tmp/hadoop-root/dfs/name
  datanode:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./config
    # volumes:
    #   - hadoop_datanode:/hadoop/dfs/data
  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./config
    volumes:
      - ./test.sh:/opt/test.sh
  nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./config
  
  spark:
    image: bitnami/spark:3.5.1
    container_name: spark-master

    depends_on:
      - namenode
      - datanode

    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - '8080:8080'
      - '7077:7077'
  spark-worker:
    image: bitnami/spark:3.5.1
    container_name: spark-worker

    depends_on:
      - spark
    
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  
  jupyter-edge-node:
    build: .
    container_name: jupyter-edge-node
    user: root
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - JUPYTER_ENABLE_LAB=yes

# volumes:
  # hadoop_namenode:
  # hadoop_datanode: