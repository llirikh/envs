{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177813d3-68d5-4278-9239-ac24235c53bf",
   "metadata": {},
   "source": [
    "## Создание Hive-таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e13bed8-b6ac-458c-a58b-bdf488de9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/15 17:05:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"from_hive\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc869b7-94fc-4a99-a75c-28ab99ddebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load = spark.sql('show databases')\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82266c00-a665-4f61-a236-3a952c198747",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49909733-f509-4333-ac95-adfadf3157e5",
   "metadata": {},
   "source": [
    "Попробуем создать из этого файла несколько паркетников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc269f9-a2ee-417e-a43f-2bebb05b0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"csv_to_many_parquets\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"NUM\", IntegerType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"CITY\", StringType(), True),\n",
    "    StructField(\"STATE\", StringType(), True),\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"hdfs://namenode:9000/data/openbeer/breweries/breweries.csv\")\n",
    "\n",
    "df = df.repartition(4)\n",
    "df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/data/openbeer/breweries_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146ab38-3028-4d00-9ec0-e864fdb30509",
   "metadata": {},
   "source": [
    "Теперь создадим таблицу в hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d26e7bc6-441c-4e06-bb81-55b2518bc3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 17:05:39 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS breweries_parquet(\n",
    "    NUM INT,\n",
    "    NAME STRING,\n",
    "    CITY STRING,\n",
    "    STATE STRING,\n",
    "    ID INT\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://namenode:9000/data/openbeer/breweries_parquet'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb74fb8-034d-453b-8d05-475175d578e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|  default|breweries_parquet|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d55593-6743-438c-b3f4-ab5ff663394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+-----+---+\n",
      "|NUM|                NAME|              CITY|STATE| ID|\n",
      "+---+--------------------+------------------+-----+---+\n",
      "| 77|    Blue Owl Brewing|            Austin|   TX| 77|\n",
      "|515|Crabtree Brewing ...|           Greeley|   CO|515|\n",
      "|222|Carton Brewing Co...|Atlantic Highlands|   NJ|222|\n",
      "|512|    Cottrell Brewing|         Pawcatuck|   CT|512|\n",
      "|332|La Cumbre Brewing...|       Albuquerque|   NM|332|\n",
      "|425|           Ciderboys|     Stevens Point|   WI|425|\n",
      "|483|Bale Breaker Brew...|            Yakima|   WA|483|\n",
      "|291|Tommyknocker Brewery|     Idaho Springs|   CO|291|\n",
      "|350|Central Coast Bre...|   San Luis Obispo|   CA|350|\n",
      "|334|The Traveler Beer...|        Burlington|   VT|334|\n",
      "|327|Half Acre Beer Co...|           Chicago|   IL|327|\n",
      "|299|Matt Brewing Company|             Utica|   NY|299|\n",
      "|311|Dirty Bucket Brew...|       Woodinville|   WA|311|\n",
      "|479|     Heavy Seas Beer|        Halethorpe|   MD|479|\n",
      "|528|Asheville Brewing...|         Asheville|   NC|528|\n",
      "|534|Mammoth Brewing C...|     Mammoth Lakes|   CA|534|\n",
      "|345|Lavery Brewing Co...|              Erie|   PA|345|\n",
      "|196|Goose Island Brew...|           Chicago|   IL|196|\n",
      "|320|Claremont Craft Ales|         Claremont|   CA|320|\n",
      "| 57|     Destihl Brewery|       Bloomington|   IL| 57|\n",
      "+---+--------------------+------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM default.breweries_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ee1750-823e-43e7-a9be-3e7190655f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9059bb-6153-4e17-bd2b-f9d4e3817b4a",
   "metadata": {},
   "source": [
    "## HDFS(Hive) -> ClickHouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d89f6ff-8a0a-45ef-88cd-1e2d1071834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.clickhouse.spark#clickhouse-spark-runtime-3.4_2.12 added as a dependency\n",
      "com.clickhouse#clickhouse-client added as a dependency\n",
      "com.clickhouse#clickhouse-http-client added as a dependency\n",
      "org.apache.httpcomponents.client5#httpclient5 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f27a855e-a5cd-4187-b5ce-9398c363148e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.clickhouse.spark#clickhouse-spark-runtime-3.4_2.12;0.8.0 in central\n",
      "\tfound com.clickhouse#clickhouse-client;0.7.0 in central\n",
      "\tfound com.clickhouse#clickhouse-data;0.7.0 in central\n",
      "\tfound com.clickhouse#clickhouse-http-client;0.7.0 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.2.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      ":: resolution report :: resolve 232ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.clickhouse#clickhouse-client;0.7.0 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-data;0.7.0 from central in [default]\n",
      "\tcom.clickhouse#clickhouse-http-client;0.7.0 from central in [default]\n",
      "\tcom.clickhouse.spark#clickhouse-spark-runtime-3.4_2.12;0.8.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.2.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f27a855e-a5cd-4187-b5ce-9398c363148e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/8ms)\n",
      "25/10/15 17:59:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/15 17:59:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"from_hive_to_clickhouse\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    # hive confs\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .enableHiveSupport()\n",
    "    # clickhouse confs\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"com.clickhouse.spark:clickhouse-spark-runtime-3.4_2.12:0.8.0\",\n",
    "            \"com.clickhouse:clickhouse-client:0.7.0\",\n",
    "            \"com.clickhouse:clickhouse-http-client:0.7.0\",\n",
    "            \"org.apache.httpcomponents.client5:httpclient5:5.2.1\",\n",
    "        ])\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.clickhouse\", \"com.clickhouse.spark.ClickHouseCatalog\")\n",
    "    .config(\"spark.sql.catalog.clickhouse.host\", \"clickhouse-server\")\n",
    "    .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "    .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "    .config(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "    .config(\"spark.sql.catalog.clickhouse.password\", \"1234qwe\")\n",
    "    .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "    .config(\"spark.clickhouse.write.format\", \"json\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a822fa01-0527-4036-87b1-e7c71099b184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NUM: integer (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM default.breweries_parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d76ac3-3b94-443a-896f-03b6c3c0e486",
   "metadata": {},
   "source": [
    "ClickHouse чувствителен к регистрам, поэтому имена колонок должны совпадать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9c72bf-e54c-46d9-b8e5-f90023340f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS clickhouse.default.test_table_2 (\n",
    "    ID INT NOT NULL,\n",
    "    NAME STRING,\n",
    "    CITY STRING\n",
    ")\n",
    "USING clickhouse\n",
    "TBLPROPERTIES (\n",
    "    'engine'='MergeTree()',\n",
    "    'order_by'='ID'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93012fa-0ebb-4975-8be8-7f9b5f7f786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 17:59:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/15 18:00:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/15 18:00:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"clickhouse.default.test_table_2\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21da5538-aa81-4d4f-8f45-7589fd2d8575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+\n",
      "| ID|                NAME|          CITY|\n",
      "+---+--------------------+--------------+\n",
      "|  0|  NorthGate Brewing |   Minneapolis|\n",
      "|  1|Against the Grain...|    Louisville|\n",
      "|  2|Jack's Abby Craft...|    Framingham|\n",
      "|  5|COAST Brewing Com...|    Charleston|\n",
      "|  7|    Tapistry Brewing|      Bridgman|\n",
      "| 15|Founders Brewing ...|  Grand Rapids|\n",
      "| 17|Tin Man Brewing C...|    Evansville|\n",
      "| 19|   Brew Link Brewing|    Plainfield|\n",
      "| 21| Three Pints Brewing|  Martinsville|\n",
      "| 23|Indiana City Brewing|  Indianapolis|\n",
      "| 24|    Burn 'Em Brewing| Michigan City|\n",
      "| 26|  Evil Czech Brewery|     Mishawaka|\n",
      "| 27|450 North Brewing...|      Columbus|\n",
      "| 29| Cedar Creek Brewery|  Seven Points|\n",
      "| 31|Boulevard Brewing...|   Kansas City|\n",
      "| 32|James Page Brewin...| Stevens Point|\n",
      "| 34|Ballast Point Bre...|     San Diego|\n",
      "| 35|Anchor Brewing Co...| San Francisco|\n",
      "| 39|Gonzo's BiggDogg ...|     Kalamazoo|\n",
      "| 41| Lost Nation Brewing|East Fairfield|\n",
      "+---+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from clickhouse.default.test_table_2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ec8cd-2aed-4a86-bd3d-2d609767d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
