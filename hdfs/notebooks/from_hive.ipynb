{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e13bed8-b6ac-458c-a58b-bdf488de9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/09 14:48:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/09 14:48:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveTest\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc869b7-94fc-4a99-a75c-28ab99ddebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| openbeer|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load = spark.sql('show databases')\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82266c00-a665-4f61-a236-3a952c198747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b4cbd-d9ba-40eb-a2d1-8fa08a192d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49909733-f509-4333-ac95-adfadf3157e5",
   "metadata": {},
   "source": [
    "Попробуем создать из этого файла несколько паркетников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc269f9-a2ee-417e-a43f-2bebb05b0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/09 14:55:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/09 14:55:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/10/09 14:56:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/09 14:56:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/09 14:56:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/09 14:56:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/09 14:57:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 4:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV сконвертирован в Parquet и сохранён в HDFS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# создаём SparkSession с Hive и HDFS\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV_to_Parquet\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# читаем CSV из HDFS\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/data/openbeer/breweries/breweries.csv\")\n",
    "\n",
    "# если нужно задать схему явно\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"NUM\", IntegerType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"CITY\", StringType(), True),\n",
    "    StructField(\"STATE\", StringType(), True),\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"hdfs://namenode:9000/data/openbeer/breweries/breweries.csv\")\n",
    "\n",
    "# разделим данные на несколько файлов (например, на 4)\n",
    "df = df.repartition(4)\n",
    "\n",
    "# сохраним в Parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/data/openbeer/breweries_parquet\")\n",
    "\n",
    "print(\"✅ CSV сконвертирован в Parquet и сохранён в HDFS!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146ab38-3028-4d00-9ec0-e864fdb30509",
   "metadata": {},
   "source": [
    "Теперь создадим таблицу в hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26e7bc6-441c-4e06-bb81-55b2518bc3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS breweries_parquet(\n",
    "    NUM INT,\n",
    "    NAME STRING,\n",
    "    CITY STRING,\n",
    "    STATE STRING,\n",
    "    ID INT\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://namenode:9000/data/openbeer/breweries_parquet'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb74fb8-034d-453b-8d05-475175d578e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "| openbeer|        breweries|      false|\n",
      "| openbeer|breweries_parquet|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверяем, что таблица появилась\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d55593-6743-438c-b3f4-ab5ff663394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+-----+---+\n",
      "|NUM|                NAME|              CITY|STATE| ID|\n",
      "+---+--------------------+------------------+-----+---+\n",
      "| 77|    Blue Owl Brewing|            Austin|   TX| 77|\n",
      "|515|Crabtree Brewing ...|           Greeley|   CO|515|\n",
      "|222|Carton Brewing Co...|Atlantic Highlands|   NJ|222|\n",
      "|512|    Cottrell Brewing|         Pawcatuck|   CT|512|\n",
      "|332|La Cumbre Brewing...|       Albuquerque|   NM|332|\n",
      "|425|           Ciderboys|     Stevens Point|   WI|425|\n",
      "|483|Bale Breaker Brew...|            Yakima|   WA|483|\n",
      "|291|Tommyknocker Brewery|     Idaho Springs|   CO|291|\n",
      "|350|Central Coast Bre...|   San Luis Obispo|   CA|350|\n",
      "|334|The Traveler Beer...|        Burlington|   VT|334|\n",
      "|327|Half Acre Beer Co...|           Chicago|   IL|327|\n",
      "|299|Matt Brewing Company|             Utica|   NY|299|\n",
      "|311|Dirty Bucket Brew...|       Woodinville|   WA|311|\n",
      "|479|     Heavy Seas Beer|        Halethorpe|   MD|479|\n",
      "|528|Asheville Brewing...|         Asheville|   NC|528|\n",
      "|534|Mammoth Brewing C...|     Mammoth Lakes|   CA|534|\n",
      "|345|Lavery Brewing Co...|              Erie|   PA|345|\n",
      "|196|Goose Island Brew...|           Chicago|   IL|196|\n",
      "|320|Claremont Craft Ales|         Claremont|   CA|320|\n",
      "| 57|     Destihl Brewery|       Bloomington|   IL| 57|\n",
      "+---+--------------------+------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверка данных\n",
    "spark.sql(\"SELECT * FROM openbeer.breweries_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32319ac7-d15f-47a4-9f73-b5b6e31f3d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
