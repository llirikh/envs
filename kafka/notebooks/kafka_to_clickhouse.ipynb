{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2b3c9-dc01-4448-89b2-5edca646b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"kafka_to_clickhouse\") \n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"org.apache.kafka:kafka-clients:3.5.1,\"\n",
    "            \"org.apache.commons:commons-pool2:2.11.1,\"\n",
    "            \"com.clickhouse:clickhouse-jdbc:0.9.1:shaded-all\"\n",
    "           )\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\")\n",
    "    # Чтобы сессия не занимала все воркеры\n",
    "    .config(\"spark.cores.max\", \"3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397b4d9-570b-4ca0-9277-9bed68f7fcbc",
   "metadata": {},
   "source": [
    "## Подготовка таблицы в ClickHouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d92a1-0fdd-4f49-8ad8-97e0530b970c",
   "metadata": {},
   "source": [
    "Выполним DDL-запрос по адресу http://localhost:8123/play\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS default.streaming_test_table\n",
    "(\n",
    "    id String,\n",
    "    name String,\n",
    "    timestamp String,\n",
    "    kafka_timestamp DateTime64(3),\n",
    "    processed_at DateTime64(3)\n",
    ")\n",
    "ENGINE = MergeTree()\n",
    "ORDER BY (processed_at, id)\n",
    "PARTITION BY toYYYYMM(processed_at);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae474f2a-9642-43e2-8831-557ac127c319",
   "metadata": {},
   "source": [
    "## Начнем запись в Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3232ec-a158-4de7-95c6-4cb66b2d77ac",
   "metadata": {},
   "source": [
    "Выполним ```python3 main.py``` в терминале"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ccdbe-96ae-4905-b99d-136a8d82b6f6",
   "metadata": {},
   "source": [
    "## Запускаем стриминг (микробатчинг)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e65054-744e-4c4b-8e43-f5779866fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"test_topic\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "parsed_df = kafka_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"),\n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    ").select(\"data.*\", \"kafka_timestamp\")\n",
    "\n",
    "processed_df = parsed_df.withColumn(\"processed_at\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfecd52-f20e-4047-ae04-ce800dd72e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write batch to ClickHouse\n",
    "def write_to_clickhouse(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Write each micro-batch to ClickHouse\n",
    "    \"\"\"\n",
    "    if not batch_df.isEmpty():\n",
    "        print(f\"\\nProcessing batch {batch_id} with {batch_df.count()} records\")\n",
    "        (\n",
    "            batch_df.write\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\")\n",
    "            .option(\"url\", \"jdbc:clickhouse://clickhouse-server:8123/default\")\n",
    "            .option(\"dbtable\", \"default.streaming_test_table\")\n",
    "            .option(\"user\", \"default\")\n",
    "            .option(\"password\", \"1234qwe\")\n",
    "            .option(\"batchsize\", \"10000\")\n",
    "            .option(\"socket_timeout\", \"300000\")\n",
    "            .option(\"numPartitions\", \"4\")append\n",
    "            .option(\"rewriteBatchedStatements\", \"true\")\n",
    "            .mode(\"append\")\n",
    "            .save()\n",
    "        )\n",
    "        print(f\"Batch {batch_id} written successfully to ClickHouse\")\n",
    "    else:\n",
    "        print(f\"Batch {batch_id} is empty, skipping\")\n",
    "\n",
    "# Write stream to ClickHouse using foreachBatch\n",
    "query = processed_df.writeStream \\\n",
    "    .foreachBatch(write_to_clickhouse) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint/kafka_to_clickhouse\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
